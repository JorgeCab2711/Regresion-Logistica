{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidad del Valle de Guatemala\n",
    "Inteligencia Artificial\n",
    "Laboratorio 4 Regresion lineal\n",
    "Jorge Caballeros Perez"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laboratorio 4 Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la libreria numpy y pandas para manejar el csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos pandas para incialmente poder leer la informacion del csv."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,  39.,   4., ...,  80.,  77.,   0.],\n",
       "       [  0.,  46.,   2., ...,  95.,  76.,   0.],\n",
       "       [  1.,  48.,   1., ...,  75.,  70.,   0.],\n",
       "       ...,\n",
       "       [  0.,  48.,   2., ...,  84.,  86.,   0.],\n",
       "       [  0.,  44.,   1., ...,  86.,  nan,   0.],\n",
       "       [  0.,  52.,   2., ...,  80., 107.,   0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Leer el archivo CSV\n",
    "framingham_df = pd.read_csv('framingham.csv')\n",
    "\n",
    "# Convertir el DataFrame de Pandas a una matriz NumPy\n",
    "framingham_np = np.array(framingham_df)\n",
    "\n",
    "framingham_np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos nuestras variables a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas de interés\n",
    "currentSmoker = framingham_np[:, 5]  # Columna 5 es currentSmoker\n",
    "TenYearCHD = framingham_np[:, 15]   # Columna 15 es TenYearCHD\n",
    "\n",
    "# Convertir a un array 2D de una sola columna\n",
    "X = currentSmoker.reshape(-1, 1)\n",
    "Y = TenYearCHD.reshape(-1, 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "\n",
    "Definimos las funciones para regresion logistica: costos, descenso del gradiente y sigmoide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cost_function(theta, X, Y):\n",
    "    m = len(Y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    J = (-1/m) * (Y.T @ np.log(h) + (1-Y).T @ np.log(1-h))\n",
    "    return J\n",
    "\n",
    "def gradient_descent(theta, X, Y, alpha, num_iters):\n",
    "    m = len(Y)\n",
    "    J_history = np.zeros((num_iters, 1))\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        h = sigmoid(X @ theta)\n",
    "        theta = theta - (alpha/m) * (X.T @ (h - Y))\n",
    "        J_history[i] = cost_function(theta, X, Y)\n",
    "\n",
    "    return theta, J_history\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mif\u001b[39;00m Y_test\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m Y_pred\u001b[39m.\u001b[39mshape:\n\u001b[0;32m     48\u001b[0m     Y_test \u001b[39m=\u001b[39m Y_test[:Y_pred\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]]\n\u001b[1;32m---> 51\u001b[0m confusionmatrix \u001b[39m=\u001b[39m confusion_matrix(Y_test, Y_pred)\n\u001b[0;32m     52\u001b[0m \u001b[39mprint\u001b[39m(confusion_matrix)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:317\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfusion_matrix\u001b[39m(\n\u001b[0;32m    233\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, normalize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    234\u001b[0m ):\n\u001b[0;32m    235\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \n\u001b[0;32m    237\u001b[0m \u001b[39m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    319\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:106\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmultilabel-indicator\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[0;32m    108\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    109\u001b[0m     y_true \u001b[39m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[1;31mValueError\u001b[0m: unknown is not supported"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Inicializar los parámetros\n",
    "theta = np.zeros((X_train.shape[1], 1))\n",
    "\n",
    "# Configurar la validación cruzada\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Ejecutar la validación cruzada\n",
    "for train_index, test_index in kf.split(X):\n",
    "        # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Normalizar los datos\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Ejecutar el descenso del gradiente\n",
    "    alpha = 0.01\n",
    "    num_iters = 1000\n",
    "    theta, J_history = gradient_descent(theta, X_train, Y_train, alpha, num_iters)\n",
    "\n",
    "    # Calcular la precisión del modelo en los datos de prueba\n",
    "    Y_pred = sigmoid(X_test @ theta)\n",
    "    Y_pred[Y_pred >= 0.5] = 1\n",
    "    Y_pred[Y_pred < 0.5] = 0\n",
    "    \n",
    "    # Get rid of NaN values in Y_test\n",
    "    nan_filter = np.isnan(Y_test)\n",
    "    X_test = X_test[~nan_filter.flatten()]\n",
    "    Y_test = Y_test[~nan_filter.flatten()]\n",
    "\n",
    "    # Get rid of NaN values in Y_pred\n",
    "    nan_filter = np.isnan(Y_pred)\n",
    "    X_test = X_test[~nan_filter.flatten()]\n",
    "    Y_pred = Y_pred[~nan_filter.flatten()]\n",
    "\n",
    "    # Ensure Y_test and Y_pred have the same number of samples\n",
    "    if Y_test.shape != Y_pred.shape:\n",
    "        Y_test = Y_test[:Y_pred.shape[0]]\n",
    "\n",
    "    \n",
    "    confusionmatrix = confusion_matrix(Y_test, Y_pred)\n",
    "    print(confusion_matrix)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
